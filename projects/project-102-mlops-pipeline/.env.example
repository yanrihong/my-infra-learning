# Project 02: End-to-End MLOps Pipeline - Environment Variables

# ==================== General Configuration ====================
PROJECT_NAME=mlops-pipeline
ENVIRONMENT=development  # development, staging, production
LOG_LEVEL=INFO

# ==================== MLflow Configuration ====================
MLFLOW_TRACKING_URI=http://mlflow:5000
MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:mlflow@postgres:5432/mlflow
MLFLOW_ARTIFACT_ROOT=s3://mlflow-artifacts  # or ./mlruns for local
MLFLOW_EXPERIMENT_NAME=default_experiment
MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts

# ==================== Airflow Configuration ====================
AIRFLOW__CORE__EXECUTOR=LocalExecutor  # Use CeleryExecutor for production
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
AIRFLOW__CORE__FERNET_KEY=<generate-with-python-cryptography-fernet>
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__WEBSERVER__SECRET_KEY=<generate-random-secret>
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
AIRFLOW_UID=50000

# ==================== Database Configuration ====================
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# MLflow Database
MLFLOW_POSTGRES_USER=mlflow
MLFLOW_POSTGRES_PASSWORD=mlflow
MLFLOW_POSTGRES_DB=mlflow

# ==================== DVC Configuration ====================
DVC_REMOTE_NAME=myremote
DVC_REMOTE_URL=s3://my-dvc-bucket/data  # or local path for testing
# AWS Credentials (if using S3)
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_DEFAULT_REGION=us-west-2

# ==================== Model Training Configuration ====================
DATA_DIR=./data
MODELS_DIR=./models
PROCESSED_DATA_DIR=./data/processed
RAW_DATA_DIR=./data/raw

# Training Parameters
BATCH_SIZE=32
LEARNING_RATE=0.001
NUM_EPOCHS=10
VALIDATION_SPLIT=0.2
RANDOM_SEED=42

# ==================== Model Deployment Configuration ====================
MODEL_SERVING_PORT=8080
MODEL_NAME=mlops-model
KUBERNETES_NAMESPACE=mlops
KUBERNETES_DEPLOYMENT_NAME=model-serving

# ==================== Monitoring Configuration ====================
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
PROMETHEUS_SCRAPE_INTERVAL=15s

# ==================== Data Pipeline Configuration ====================
DATA_SOURCE_URL=https://example.com/dataset.csv
DATA_INGESTION_SCHEDULE=0 0 * * *  # Daily at midnight
DATA_VALIDATION_THRESHOLD=0.95

# ==================== Model Registry Configuration ====================
MODEL_STAGE_STAGING=Staging
MODEL_STAGE_PRODUCTION=Production
MODEL_PROMOTION_THRESHOLD=0.85  # Minimum accuracy for promotion

# ==================== CI/CD Configuration ====================
DOCKER_REGISTRY=docker.io
DOCKER_IMAGE_NAME=mlops-pipeline
DOCKER_IMAGE_TAG=latest

# ==================== Object Storage (MinIO for local development) ====================
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_ENDPOINT=http://minio:9000
MINIO_BUCKET=mlflow-artifacts

# ==================== Redis (for Airflow Celery) ====================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# ==================== Notification Configuration ====================
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
EMAIL_NOTIFICATIONS_ENABLED=False
EMAIL_SMTP_HOST=smtp.gmail.com
EMAIL_SMTP_PORT=587
EMAIL_FROM=mlops-pipeline@example.com

# ==================== Feature Engineering ====================
FEATURE_STORE_ENABLED=False
# FEAST_REPO_PATH=./feature_repo

# ==================== Experiment Tracking ====================
TRACK_SYSTEM_METRICS=True
LOG_EVERY_N_STEPS=10
SAVE_CHECKPOINT_EVERY_N_EPOCHS=5

# ==================== Data Quality ====================
DATA_QUALITY_CHECKS_ENABLED=True
OUTLIER_DETECTION_ENABLED=True
SCHEMA_VALIDATION_ENABLED=True
