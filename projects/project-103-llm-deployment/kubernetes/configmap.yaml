apiVersion: v1
kind: ConfigMap
metadata:
  name: llm-config
data:
  # TODO: Model configuration
  model-config.yaml: |
    model_name: "meta-llama/Llama-2-7b-chat-hf"
    model_path: "/models"
    max_model_len: 4096
    tensor_parallel_size: 1
    dtype: "float16"

  # TODO: Generation defaults
  generation-config.yaml: |
    max_tokens: 512
    temperature: 0.7
    top_p: 0.95
    top_k: 50
    presence_penalty: 0.0
    frequency_penalty: 0.0

  # TODO: API configuration
  api-config.yaml: |
    host: "0.0.0.0"
    port: 8000
    workers: 1
    timeout: 300
    max_batch_size: 8

  # TODO: Logging configuration
  logging-config.yaml: |
    version: 1
    formatters:
      default:
        format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: default
        stream: ext://sys.stdout
    root:
      level: INFO
      handlers: [console]
