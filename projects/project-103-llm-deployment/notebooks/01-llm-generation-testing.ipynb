{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Generation Testing\n",
    "\n",
    "This notebook demonstrates how to test and evaluate LLM text generation capabilities.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Load and test LLM models\n",
    "- Experiment with generation parameters\n",
    "- Measure inference performance\n",
    "- Compare different models/configurations\n",
    "\n",
    "## TODO Tasks\n",
    "\n",
    "1. Load your LLM model (vLLM, Hugging Face, or API)\n",
    "2. Test basic text generation\n",
    "3. Experiment with temperature, top_p, top_k\n",
    "4. Benchmark latency and throughput\n",
    "5. Compare batch vs streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# TODO: Import your LLM library\n",
    "# Examples:\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import openai\n",
    "# from vllm import LLM, SamplingParams\n",
    "\n",
    "# Import utilities\n",
    "from utils import test_llm_generation, benchmark_inference_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"  # TODO: Change to your model\n",
    "MAX_TOKENS = 200\n",
    "TEMPERATURE = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model\n",
    "\n",
    "TODO: Load your LLM model here. Choose one approach:\n",
    "\n",
    "### Option A: Hugging Face Transformers\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "```\n",
    "\n",
    "### Option B: vLLM (faster inference)\n",
    "```python\n",
    "llm = LLM(model=MODEL_NAME)\n",
    "```\n",
    "\n",
    "### Option C: OpenAI API\n",
    "```python\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load your model\n",
    "# Example for vLLM:\n",
    "# llm = LLM(model=MODEL_NAME)\n",
    "\n",
    "print(\"TODO: Implement model loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Text Generation\n",
    "\n",
    "Test basic text generation with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "prompt = \"Explain the concept of machine learning in simple terms:\"\n",
    "\n",
    "# TODO: Generate text\n",
    "# Example for vLLM:\n",
    "# sampling_params = SamplingParams(temperature=TEMPERATURE, max_tokens=MAX_TOKENS)\n",
    "# outputs = llm.generate([prompt], sampling_params)\n",
    "# generated_text = outputs[0].outputs[0].text\n",
    "\n",
    "# TODO: Print results\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nGenerated text: TODO - implement generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parameter Experimentation\n",
    "\n",
    "Experiment with different generation parameters to understand their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different temperatures\n",
    "temperatures = [0.0, 0.5, 0.7, 1.0, 1.5]\n",
    "\n",
    "prompt = \"Write a creative story about a robot:\"\n",
    "\n",
    "print(\"Testing different temperatures:\\n\")\n",
    "for temp in temperatures:\n",
    "    # TODO: Generate with each temperature\n",
    "    # sampling_params = SamplingParams(temperature=temp, max_tokens=100)\n",
    "    # output = llm.generate([prompt], sampling_params)\n",
    "    \n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(\"TODO - implement generation\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Benchmarking\n",
    "\n",
    "Measure inference latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark latency\n",
    "test_prompts = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain neural networks.\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Describe natural language processing.\"\n",
    "]\n",
    "\n",
    "latencies = []\n",
    "tokens_generated = []\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TODO: Generate text and measure time\n",
    "    # output = llm.generate([prompt], sampling_params)\n",
    "    \n",
    "    latency = time.time() - start_time\n",
    "    latencies.append(latency)\n",
    "    \n",
    "    # TODO: Count tokens\n",
    "    # num_tokens = len(output[0].outputs[0].token_ids)\n",
    "    # tokens_generated.append(num_tokens)\n",
    "\n",
    "# Calculate statistics\n",
    "# TODO: Implement actual calculations\n",
    "avg_latency = 0.0  # np.mean(latencies)\n",
    "p95_latency = 0.0  # np.percentile(latencies, 95)\n",
    "avg_tokens = 0.0   # np.mean(tokens_generated)\n",
    "throughput = 0.0   # avg_tokens / avg_latency\n",
    "\n",
    "print(f\"Average latency: {avg_latency:.3f}s\")\n",
    "print(f\"P95 latency: {p95_latency:.3f}s\")\n",
    "print(f\"Average tokens: {avg_tokens:.0f}\")\n",
    "print(f\"Throughput: {throughput:.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Latency Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot latency distribution\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(latencies, bins=20, edgecolor='black')\n",
    "# plt.xlabel('Latency (seconds)')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Inference Latency Distribution')\n",
    "# plt.axvline(avg_latency, color='r', linestyle='--', label=f'Mean: {avg_latency:.3f}s')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "print(\"TODO: Implement latency visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch vs Streaming Comparison\n",
    "\n",
    "Compare batch inference vs streaming for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test batch inference\n",
    "batch_prompts = test_prompts[:4]\n",
    "\n",
    "# Batch processing\n",
    "start_time = time.time()\n",
    "# TODO: Generate for all prompts at once\n",
    "# outputs = llm.generate(batch_prompts, sampling_params)\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "print(f\"Batch processing: TODO - implement\")\n",
    "print(f\"Time: {batch_time:.3f}s\")\n",
    "print(f\"Average per prompt: {batch_time / len(batch_prompts):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "TODO: Document your findings:\n",
    "- What temperature works best for your use case?\n",
    "- What's the latency/throughput trade-off?\n",
    "- Is batch processing beneficial?\n",
    "- What optimizations can you apply?\n",
    "\n",
    "### Recommendations\n",
    "TODO: Based on your experiments:\n",
    "1. Recommended temperature setting: __\n",
    "2. Optimal batch size: __\n",
    "3. Expected latency: __\n",
    "4. Cost per 1000 requests: __\n",
    "\n",
    "### Next Steps\n",
    "1. Move to `02-rag-experimentation.ipynb` to test RAG\n",
    "2. Implement findings in production code\n",
    "3. Set up monitoring for these metrics\n",
    "4. Configure autoscaling based on latency targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
