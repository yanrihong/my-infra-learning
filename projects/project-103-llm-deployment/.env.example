# LLM Deployment Platform - Environment Configuration
# Copy this file to .env and fill in your values

# ============================================================================
# LLM Configuration
# ============================================================================

# Model Selection
LLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.2
# Alternatives: meta-llama/Llama-2-7b-chat-hf, tiiuae/falcon-7b-instruct
LLM_MODEL_PATH=/models/llm  # Local path to cached model weights
DOWNLOAD_MODEL_ON_STARTUP=true

# Model Optimization
QUANTIZATION_METHOD=awq  # Options: awq, gptq, bitsandbytes, none
LOAD_IN_8BIT=false       # Enable 8-bit quantization (reduces VRAM)
LOAD_IN_4BIT=false       # Enable 4-bit quantization (further reduction)
USE_FLASH_ATTENTION=true # Flash Attention 2 for faster inference

# Inference Configuration
MAX_MODEL_LENGTH=4096    # Maximum sequence length
GPU_MEMORY_UTILIZATION=0.9  # Fraction of GPU memory to use (0.0-1.0)
TENSOR_PARALLEL_SIZE=1   # Number of GPUs for tensor parallelism
PIPELINE_PARALLEL_SIZE=1 # Number of GPUs for pipeline parallelism
MAX_NUM_SEQS=256        # Maximum number of sequences in batch

# Generation Defaults
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=0.9
DEFAULT_TOP_K=50
DEFAULT_MAX_TOKENS=512
ENABLE_STREAMING=true

# ============================================================================
# RAG Configuration
# ============================================================================

# Embedding Model
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# Alternatives: BAAI/bge-large-en-v1.5, intfloat/e5-large-v2
EMBEDDING_DIMENSION=384  # Dimension of embedding vectors
EMBEDDING_BATCH_SIZE=32

# Vector Database Selection (choose one)
VECTOR_DB_TYPE=milvus    # Options: milvus, weaviate, chroma, qdrant

# Milvus Configuration
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_USER=root
MILVUS_PASSWORD=Milvus
MILVUS_COLLECTION_NAME=documents
MILVUS_INDEX_TYPE=IVF_FLAT  # Options: FLAT, IVF_FLAT, IVF_SQ8, HNSW

# Weaviate Configuration (alternative)
WEAVIATE_URL=http://localhost:8080
WEAVIATE_API_KEY=
WEAVIATE_CLASS_NAME=Document

# ChromaDB Configuration (alternative - lightweight)
CHROMA_PERSIST_DIRECTORY=/data/chromadb
CHROMA_COLLECTION_NAME=documents

# Retrieval Configuration
TOP_K_RETRIEVAL=5        # Number of documents to retrieve
SIMILARITY_THRESHOLD=0.7 # Minimum similarity score
RERANK_ENABLED=false     # Enable reranking of retrieved documents
RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Chunking Strategy
CHUNK_SIZE=500          # Characters per chunk
CHUNK_OVERLAP=50        # Overlap between chunks
CHUNKING_METHOD=recursive  # Options: fixed, recursive, semantic

# ============================================================================
# API Configuration
# ============================================================================

# Server Settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1           # Number of Uvicorn workers (1 for GPU workloads)
API_RELOAD=false        # Auto-reload on code changes (dev only)
LOG_LEVEL=info          # Options: debug, info, warning, error

# API Security
API_KEY_ENABLED=true
API_KEYS=sk-test-key-1,sk-test-key-2  # Comma-separated API keys
JWT_SECRET_KEY=your-secret-key-change-this-in-production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=60

# Rate Limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100  # Requests per minute per API key
RATE_LIMIT_TOKENS=50000  # Tokens per minute per API key

# CORS Configuration
CORS_ENABLED=true
CORS_ORIGINS=http://localhost:3000,https://yourdomain.com

# ============================================================================
# Monitoring & Observability
# ============================================================================

# Prometheus
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=8001
METRICS_NAMESPACE=llm_platform

# OpenTelemetry
OTEL_ENABLED=false
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
OTEL_SERVICE_NAME=llm-deployment-platform

# Cost Tracking
COST_TRACKING_ENABLED=true
COST_PER_1K_INPUT_TOKENS=0.001   # USD per 1K input tokens
COST_PER_1K_OUTPUT_TOKENS=0.002  # USD per 1K output tokens
COST_PER_GPU_HOUR=1.50           # USD per GPU hour

# Logging
LOG_FILE=/var/log/llm-platform/app.log
LOG_ROTATION=1 day
LOG_RETENTION=30 days
LOG_JSON_FORMAT=true

# ============================================================================
# GPU Configuration
# ============================================================================

# GPU Selection
CUDA_VISIBLE_DEVICES=0   # GPU IDs to use (comma-separated)
GPU_TYPE=A100            # For monitoring/cost tracking

# GPU Memory Management
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
CUDA_LAUNCH_BLOCKING=0

# ============================================================================
# Caching
# ============================================================================

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
CACHE_TTL=3600          # Cache TTL in seconds
CACHE_ENABLED=true

# Response Caching
CACHE_RESPONSES=true
CACHE_EMBEDDINGS=true   # Cache document embeddings

# ============================================================================
# Database (Metadata Storage)
# ============================================================================

# PostgreSQL for document metadata
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=llm_platform
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
DATABASE_POOL_SIZE=10

# ============================================================================
# Document Ingestion
# ============================================================================

# File Upload
MAX_UPLOAD_SIZE_MB=50
ALLOWED_FILE_TYPES=pdf,txt,docx,md,html
UPLOAD_DIR=/data/uploads

# Processing
ASYNC_INGESTION=true
INGESTION_BATCH_SIZE=10
OCR_ENABLED=false        # Enable OCR for scanned PDFs (requires Tesseract)

# ============================================================================
# Kubernetes & Deployment
# ============================================================================

# Service Configuration
ENVIRONMENT=development  # Options: development, staging, production
SERVICE_NAME=llm-deployment-platform
NAMESPACE=default

# Health Checks
HEALTH_CHECK_INTERVAL=30  # Seconds
READINESS_TIMEOUT=300     # Seconds to wait for model loading

# Autoscaling
MIN_REPLICAS=1
MAX_REPLICAS=5
TARGET_GPU_UTILIZATION=70  # Percentage

# ============================================================================
# Feature Flags
# ============================================================================

ENABLE_RAG=true
ENABLE_STREAMING=true
ENABLE_BATCH_INFERENCE=false
ENABLE_PROMPT_CACHING=true
ENABLE_SAFETY_FILTERS=true

# ============================================================================
# Experimental Features
# ============================================================================

# Speculative Decoding
ENABLE_SPECULATIVE_DECODING=false
DRAFT_MODEL_NAME=

# Multi-LoRA Support
ENABLE_LORA=false
LORA_ADAPTERS_DIR=/models/lora
